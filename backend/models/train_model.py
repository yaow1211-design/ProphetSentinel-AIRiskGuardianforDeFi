"""
MLæ¨¡å‹è®­ç»ƒè„šæœ¬
"""
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pickle
import os
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def generate_synthetic_training_data(n_samples=1000):
    """
    ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®
    
    ç‰¹å¾:
    - volume_24h: 24å°æ—¶äº¤æ˜“é‡
    - liquidity_change: æµåŠ¨æ€§å˜åŒ–ç‡
    - whale_transfers: é²¸é±¼è½¬ç§»æ¬¡æ•°
    - holder_concentration: æŒæœ‰é›†ä¸­åº¦
    
    æ ‡ç­¾:
    - 0: ä½é£é™©
    - 1: é«˜é£é™©
    """
    logger.info(f"ğŸ”§ ç”Ÿæˆ {n_samples} æ¡åˆæˆè®­ç»ƒæ•°æ®...")
    
    np.random.seed(42)
    
    data = []
    
    for _ in range(n_samples):
        # éšæœºå†³å®šæ˜¯å¦ä¸ºé«˜é£é™©åè®®
        is_high_risk = np.random.rand() > 0.7  # 30%é«˜é£é™©
        
        if is_high_risk:
            # é«˜é£é™©åè®®ç‰¹å¾
            volume = np.random.uniform(1000000, 10000000)
            liquidity_change = np.random.uniform(-0.5, -0.1)  # æµåŠ¨æ€§ä¸‹é™
            whale_transfers = np.random.randint(5, 15)  # é¢‘ç¹é²¸é±¼æ´»åŠ¨
            holder_concentration = np.random.uniform(0.7, 0.95)  # é«˜é›†ä¸­åº¦
            label = 1
        else:
            # ä½é£é™©åè®®ç‰¹å¾
            volume = np.random.uniform(5000000, 100000000)
            liquidity_change = np.random.uniform(-0.1, 0.3)  # æµåŠ¨æ€§ç¨³å®šæˆ–å¢é•¿
            whale_transfers = np.random.randint(0, 5)  # å°‘é‡é²¸é±¼æ´»åŠ¨
            holder_concentration = np.random.uniform(0.2, 0.6)  # åˆ†æ•£æŒæœ‰
            label = 0
        
        data.append({
            'volume_24h': volume,
            'liquidity_change': liquidity_change,
            'whale_transfers': whale_transfers,
            'holder_concentration': holder_concentration,
            'risk_label': label
        })
    
    df = pd.DataFrame(data)
    logger.info(f"âœ… æ•°æ®ç”Ÿæˆå®Œæˆ: {len(df)} æ¡è®°å½•")
    logger.info(f"   é«˜é£é™©æ ·æœ¬: {sum(df['risk_label'] == 1)} ({sum(df['risk_label'] == 1)/len(df)*100:.1f}%)")
    logger.info(f"   ä½é£é™©æ ·æœ¬: {sum(df['risk_label'] == 0)} ({sum(df['risk_label'] == 0)/len(df)*100:.1f}%)")
    
    return df

def train_risk_model(df):
    """è®­ç»ƒRandomForestæ¨¡å‹"""
    logger.info("ğŸ¤– å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    
    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    feature_columns = ['volume_24h', 'liquidity_change', 'whale_transfers', 'holder_concentration']
    X = df[feature_columns]
    y = df['risk_label']
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    logger.info(f"è®­ç»ƒé›†: {len(X_train)} æ¡, æµ‹è¯•é›†: {len(X_test)} æ¡")
    
    # è®­ç»ƒRandomForest
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        random_state=42,
        class_weight='balanced'  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
    )
    
    model.fit(X_train, y_train)
    
    # è¯„ä¼°æ¨¡å‹
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    logger.info(f"\n{'='*50}")
    logger.info(f"ğŸ¯ æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    logger.info(f"å‡†ç¡®ç‡: {accuracy:.2%}")
    logger.info(f"\nåˆ†ç±»æŠ¥å‘Š:\n{classification_report(y_test, y_pred)}")
    logger.info(f"{'='*50}\n")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': feature_columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    logger.info("ğŸ“Š ç‰¹å¾é‡è¦æ€§:")
    for _, row in feature_importance.iterrows():
        logger.info(f"   {row['feature']}: {row['importance']:.4f}")
    
    return model, accuracy

def save_model(model, path='backend/models/risk_model.pkl'):
    """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    
    with open(path, 'wb') as f:
        pickle.dump(model, f)
    
    logger.info(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {path}")

def save_training_data(df, path='data/processed/training_data.csv'):
    """ä¿å­˜è®­ç»ƒæ•°æ®"""
    os.makedirs(os.path.dirname(path), exist_ok=True)
    df.to_csv(path, index=False)
    logger.info(f"ğŸ’¾ è®­ç»ƒæ•°æ®å·²ä¿å­˜: {path}")

def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    logger.info("ğŸš€ Prophet Sentinel - æ¨¡å‹è®­ç»ƒå¼€å§‹\n")
    
    # 1. ç”Ÿæˆè®­ç»ƒæ•°æ®
    df = generate_synthetic_training_data(n_samples=2000)
    
    # 2. ä¿å­˜è®­ç»ƒæ•°æ®
    save_training_data(df)
    
    # 3. è®­ç»ƒæ¨¡å‹
    model, accuracy = train_risk_model(df)
    
    # 4. ä¿å­˜æ¨¡å‹
    save_model(model)
    
    # 5. æµ‹è¯•é¢„æµ‹
    logger.info("\nğŸ§ª æµ‹è¯•é¢„æµ‹:")
    test_cases = [
        {
            'name': 'ä½é£é™©åè®®',
            'metrics': [50000000, 0.1, 2, 0.4]
        },
        {
            'name': 'ä¸­é£é™©åè®®',
            'metrics': [10000000, -0.15, 5, 0.65]
        },
        {
            'name': 'é«˜é£é™©åè®®',
            'metrics': [2000000, -0.35, 12, 0.88]
        }
    ]
    
    for test in test_cases:
        proba = model.predict_proba([test['metrics']])[0]
        risk_score = int(proba[1] * 100)
        logger.info(f"   {test['name']}: é£é™©åˆ†æ•° = {risk_score}")
    
    logger.info(f"\nâœ… è®­ç»ƒå®Œæˆ! å‡†ç¡®ç‡: {accuracy:.2%}")
    logger.info("å¯ä»¥è¿è¡Œ Flask åº”ç”¨äº†: python backend/app.py\n")

if __name__ == '__main__':
    main()





